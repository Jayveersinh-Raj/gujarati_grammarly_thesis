{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Function to calculate glue"
      ],
      "metadata": {
        "id": "yZyAeLEGXL7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk.translate.gleu_score as gleu\n",
        "\n",
        "# Function to calculate GLEU score\n",
        "def calculate_gleu(original, corrected, reference):\n",
        "    original_tokens = original.split()\n",
        "    corrected_tokens = corrected.split()\n",
        "    reference_tokens = [reference.split()]\n",
        "\n",
        "    gleu_score = gleu.corpus_gleu([reference_tokens], [corrected_tokens])\n",
        "    return gleu_score"
      ],
      "metadata": {
        "id": "zx3rUkIGXKil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to get the gleu score from the given csv with format:\n",
        "- $1^{st}$ colum (Input Sentence) : sentence passed for correction with potential errors\n",
        "- $2^{nd}$ column (Ground Truth): The correct correction of the corresponding input sentence\n",
        "- $3^{rd}$ column (Generated sentence): Model corrected generated sentence"
      ],
      "metadata": {
        "id": "yx0VaWzwXT84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gleu for small uncurated"
      ],
      "metadata": {
        "id": "fogbJmRRWQmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV file\n",
        "def gleu_from_csv(csv_file: str) -> float:\n",
        "  \"\"\"\n",
        "  # Function to get the gleu score from the given csv with format:\n",
        "- $1^{st}$ colum (Input Sentence) : sentence passed for correction with potential errors\n",
        "- $2^{nd}$ column (Ground Truth): The correct correction of the corresponding input sentence\n",
        "- $3^{rd} column (Generated sentence): Model corrected generated sentence\n",
        "\n",
        "  Parameters\n",
        "  -----------\n",
        "  csv_file: the path to the csv file\n",
        "\n",
        "  Returns\n",
        "  -----------\n",
        "  gleu_score: Average gleu score\n",
        "  \"\"\"\n",
        "\n",
        "  original_sentences = []\n",
        "  ground_truth_corrections = []\n",
        "  model_corrected_sentences = []\n",
        "\n",
        "  with open(csv_file, newline='', encoding='utf-8') as csvfile:\n",
        "      csv_reader = csv.reader(csvfile)\n",
        "      next(csv_reader)  # Skip header if exists\n",
        "      for row in csv_reader:\n",
        "          original_sentences.append(row[0])\n",
        "          ground_truth_corrections.append(row[1])\n",
        "          model_corrected_sentences.append(row[2])\n",
        "\n",
        "  # Calculate GLEU scores\n",
        "  total_gleu_score = 0\n",
        "  num_sentences = len(original_sentences)\n",
        "\n",
        "  for i in range(num_sentences):\n",
        "      gleu_score = calculate_gleu(original_sentences[i], model_corrected_sentences[i], ground_truth_corrections[i])\n",
        "      total_gleu_score += gleu_score\n",
        "\n",
        "  average_gleu_score = total_gleu_score / num_sentences\n",
        "  return average_gleu_score"
      ],
      "metadata": {
        "id": "Ws3_okWFWlnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For small"
      ],
      "metadata": {
        "id": "rJcCSlRcYim4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average GLEU score for small model:\", gleu_from_csv(\"/content/pred_small.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySVHktrsYkkG",
        "outputId": "df24430e-a6ce-4d2f-f514-20f3216ea13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average GLEU score for small model: 0.6724733573174159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For base"
      ],
      "metadata": {
        "id": "4sIDVioFXFZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average GLEU score for base model:\", gleu_from_csv(\"/content/processed_pred_base.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2mOBebCXRWi",
        "outputId": "4c4b96a2-6c01-4eb3-950c-b7815430c3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average GLEU score for base model: 0.931907807313693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For large"
      ],
      "metadata": {
        "id": "OpfHZ-F-Y32i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average GLEU score for large model:\", gleu_from_csv(\"/content/processed_pred_large.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox-b5016Y8NK",
        "outputId": "70c323de-deca-4688-93ed-9bd7ecf8b3e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average GLEU score for large model: 0.8961809490807884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For curated datasets"
      ],
      "metadata": {
        "id": "TsSOhWyxaPqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average GLEU score for small model:\", gleu_from_csv(\"/content/curated_small_pred.csv\"))\n",
        "print(\"Average GLEU score for base model:\", gleu_from_csv(\"/content/curated_base_pred.csv\"))\n",
        "print(\"Average GLEU score for large model:\", gleu_from_csv(\"/content/curated_large_pred.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP-5EkMlaRkl",
        "outputId": "d054a6f5-68c0-47cc-f965-9a5e2d64fcd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average GLEU score for small model: 0.6259194060631131\n",
            "Average GLEU score for base model: 0.9902114347752021\n",
            "Average GLEU score for large model: 0.9318732251915798\n"
          ]
        }
      ]
    }
  ]
}
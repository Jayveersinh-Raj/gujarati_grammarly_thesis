{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kZr0kHmf_shj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_small = pd.read_csv(\"/content/curated_small_pred.csv\")\n",
        "df_base = pd.read_csv(\"/content/curated_base_pred.csv\")\n",
        "df_large = pd.read_csv(\"/content/curated_large_pred.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate metrics function, Precision, Recall, F1"
      ],
      "metadata": {
        "id": "tLc5SC94_5j_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(ground_truth, input_sentence, generated_sentence):\n",
        "    ground_truth = ground_truth.tolist()\n",
        "    input_sentence = input_sentence.tolist()\n",
        "    generated_sentence = generated_sentence.tolist()\n",
        "\n",
        "    # Identify wrong words in input sentence and ground truth\n",
        "    wrong_words = [(gt_word, input_word) for gt_word, input_word in zip(ground_truth, input_sentence) if gt_word != input_word]\n",
        "\n",
        "    # Count the number of wrong words corrected by the model\n",
        "    corrected_by_model = sum(1 for gt_word, input_word in wrong_words if generated_sentence[input_sentence.index(input_word)] == gt_word)\n",
        "\n",
        "    # Calculate metrics\n",
        "    total_wrong_words = len(wrong_words)\n",
        "    precision = corrected_by_model / total_wrong_words if total_wrong_words > 0 else 0\n",
        "    recall = corrected_by_model / len(ground_truth)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"Total Wrong Words\": total_wrong_words,\n",
        "        \"Corrected by Model\": corrected_by_model,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1 Score\": f1_score\n",
        "    }"
      ],
      "metadata": {
        "id": "c9O_DKHA_7j5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Small"
      ],
      "metadata": {
        "id": "xgfXa8FOAA05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth = df_small[\"Ground Truth\"]\n",
        "input_sentence = df_small[\"Input Sentence\"]\n",
        "generated_sentence = df_small[\"Generated Sentence\"]\n",
        "\n",
        "metrics = calculate_metrics(ground_truth, input_sentence, generated_sentence)\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGZ4JpkgACD0",
        "outputId": "d9fe9579-b578-482c-c764-601a21d1d196"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Total Wrong Words': 3000, 'Corrected by Model': 2025, 'Precision': 0.675, 'Recall': 0.675, 'F1 Score': 0.675}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base"
      ],
      "metadata": {
        "id": "QY3p41FOADQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth = df_base[\"Ground Truth\"]\n",
        "input_sentence = df_base[\"Input Sentence\"]\n",
        "generated_sentence = df_base[\"Generated Sentence\"]\n",
        "\n",
        "metrics = calculate_metrics(ground_truth, input_sentence, generated_sentence)\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KirHovj7AGIn",
        "outputId": "7e09d76d-230a-4891-cbc3-6ecfa5eaced6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Total Wrong Words': 3000, 'Corrected by Model': 2745, 'Precision': 0.915, 'Recall': 0.915, 'F1 Score': 0.915}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large"
      ],
      "metadata": {
        "id": "qAb02FNgAJl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth = df_large[\"Ground Truth\"]\n",
        "input_sentence = df_large[\"Input Sentence\"]\n",
        "generated_sentence = df_large[\"Generated Sentence\"]\n",
        "\n",
        "metrics = calculate_metrics(ground_truth, input_sentence, generated_sentence)\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl2xwXbxALQr",
        "outputId": "44d0e38a-4d47-44e6-9516-20d9fb9c6354"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Total Wrong Words': 3000, 'Corrected by Model': 1977, 'Precision': 0.659, 'Recall': 0.659, 'F1 Score': 0.659}\n"
          ]
        }
      ]
    }
  ]
}
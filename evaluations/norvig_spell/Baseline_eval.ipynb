{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Function for GLEU"
      ],
      "metadata": {
        "id": "soU_4A6btNEB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "njI9JQSCsnpW"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import nltk.translate.gleu_score as gleu\n",
        "\n",
        "# Function to calculate GLEU score\n",
        "def calculate_gleu(original, corrected, reference):\n",
        "    original_tokens = original.split()\n",
        "    corrected_tokens = corrected.split()\n",
        "    reference_tokens = [reference.split()]\n",
        "\n",
        "    gleu_score = gleu.corpus_gleu([reference_tokens], [corrected_tokens])\n",
        "    return gleu_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to get the gleu score from the given csv with format:\n",
        "- $1^{st}$ colum (Input Sentence) : sentence passed for correction with potential errors\n",
        "- $2^{nd}$ column (Ground Truth): The correct correction of the corresponding input sentence\n",
        "- $3^{rd}$ column (Generated sentence): Model corrected generated sentence"
      ],
      "metadata": {
        "id": "P1Kq78qFtQd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV file\n",
        "def gleu_from_csv(csv_file: str) -> float:\n",
        "  \"\"\"\n",
        "  # Function to get the gleu score from the given csv with format:\n",
        "- $1^{st}$ colum (Input Sentence) : sentence passed for correction with potential errors\n",
        "- $2^{nd}$ column (Ground Truth): The correct correction of the corresponding input sentence\n",
        "- $3^{rd} column (Generated sentence): Model corrected generated sentence\n",
        "\n",
        "  Parameters\n",
        "  -----------\n",
        "  csv_file: the path to the csv file\n",
        "\n",
        "  Returns\n",
        "  -----------\n",
        "  gleu_score: Average gleu score\n",
        "  \"\"\"\n",
        "\n",
        "  original_sentences = []\n",
        "  ground_truth_corrections = []\n",
        "  model_corrected_sentences = []\n",
        "\n",
        "  with open(csv_file, newline='', encoding='utf-8') as csvfile:\n",
        "      csv_reader = csv.reader(csvfile)\n",
        "      next(csv_reader)  # Skip header if exists\n",
        "      for row in csv_reader:\n",
        "          original_sentences.append(row[0])\n",
        "          ground_truth_corrections.append(row[1])\n",
        "          model_corrected_sentences.append(row[2])\n",
        "\n",
        "  # Calculate GLEU scores\n",
        "  total_gleu_score = 0\n",
        "  num_sentences = len(original_sentences)\n",
        "\n",
        "  for i in range(num_sentences):\n",
        "      gleu_score = calculate_gleu(original_sentences[i], model_corrected_sentences[i], ground_truth_corrections[i])\n",
        "      total_gleu_score += gleu_score\n",
        "\n",
        "  average_gleu_score = total_gleu_score / num_sentences\n",
        "  return average_gleu_score"
      ],
      "metadata": {
        "id": "5a5ZjBIrtWCA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average GLEU score for norvig spell checking:\", gleu_from_csv(\"norvig_pred.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPJSTXD2tbdW",
        "outputId": "77df89d6-8d9a-4f20-8bc5-52886bb485e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average GLEU score for norvig spell checking: 0.20518247006575785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Precision"
      ],
      "metadata": {
        "id": "au_nd0QutqDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(ground_truth, input_sentence, generated_sentence):\n",
        "    ground_truth = ground_truth.tolist()\n",
        "    input_sentence = input_sentence.tolist()\n",
        "    generated_sentence = generated_sentence.tolist()\n",
        "\n",
        "    # Identify wrong words in input sentence and ground truth\n",
        "    wrong_words = [(gt_word, input_word) for gt_word, input_word in zip(ground_truth, input_sentence) if gt_word != input_word]\n",
        "\n",
        "    # Count the number of wrong words corrected by the model\n",
        "    corrected_by_model = sum(1 for gt_word, input_word in wrong_words if generated_sentence[input_sentence.index(input_word)] == gt_word)\n",
        "\n",
        "    # Calculate metrics\n",
        "    total_wrong_words = len(wrong_words)\n",
        "    precision = corrected_by_model / total_wrong_words if total_wrong_words > 0 else 0\n",
        "    recall = corrected_by_model / len(ground_truth)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"Total Wrong Words\": total_wrong_words,\n",
        "        \"Corrected by Model\": corrected_by_model,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1 Score\": f1_score\n",
        "    }"
      ],
      "metadata": {
        "id": "1Idw0WYHtuiu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"norvig_pred.csv\")\n",
        "\n",
        "ground_truth = df[\"Grount Truth\"]\n",
        "input_sentence = df[\"Input Sentence\"]\n",
        "generated_sentence = df[\"Generated Sentence\"]\n",
        "\n",
        "metrics = calculate_metrics(ground_truth, input_sentence, generated_sentence)\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3KZJO7jt1Xe",
        "outputId": "27cd7535-5d4b-4a37-e69f-37f4936b7866"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Total Wrong Words': 5372, 'Corrected by Model': 0, 'Precision': 0.0, 'Recall': 0.0, 'F1 Score': 0}\n"
          ]
        }
      ]
    }
  ]
}